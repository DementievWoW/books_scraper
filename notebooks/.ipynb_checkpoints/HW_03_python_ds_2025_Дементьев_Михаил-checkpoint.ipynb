{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca0353b",
   "metadata": {
    "id": "aca0353b"
   },
   "source": [
    "# Домашнее задание 3. Парсинг, Git и тестирование на Python\n",
    "\n",
    "**Цели задания:**\n",
    "\n",
    "* Освоить базовые подходы к web-scraping с библиотеками `requests` и `BeautisulSoup`: навигация по страницам, извлечение HTML-элементов, парсинг.\n",
    "* Научиться автоматизировать задачи с использованием библиотеки `schedule`.\n",
    "* Попрактиковаться в использовании Git и оформлении проектов на GitHub.\n",
    "* Написать и запустить простые юнит-тесты с использованием `pytest`.\n",
    "\n",
    "\n",
    "В этом домашнем задании вы разработаете систему для автоматического сбора данных о книгах с сайта [Books to Scrape](http://books.toscrape.com). Нужно реализовать функции для парсинга всех страниц сайта, извлечения информации о книгах, автоматического ежедневного запуска задачи и сохранения результата.\n",
    "\n",
    "Важной частью задания станет оформление проекта: вы создадите репозиторий на GitHub, оформите `README.md`, добавите артефакты (код, данные, отчеты) и напишете базовые тесты на `pytest`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "K3JMV0qwmA_q",
   "metadata": {
    "id": "K3JMV0qwmA_q"
   },
   "outputs": [],
   "source": [
    "!pip install -q schedule pytest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873d4904",
   "metadata": {
    "id": "873d4904"
   },
   "outputs": [],
   "source": [
    "# Библиотеки, которые могут вам понадобиться\n",
    "# При необходимости расширяйте список\n",
    "import time\n",
    "import requests\n",
    "import schedule\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unTvsWaegHdj",
   "metadata": {
    "id": "unTvsWaegHdj"
   },
   "source": [
    "## Задание 1. Сбор данных об одной книге (20 баллов)\n",
    "\n",
    "В этом задании мы начнем подготовку скрипта для парсинга информации о книгах со страниц каталога сайта [Books to Scrape](https://books.toscrape.com/).\n",
    "\n",
    "Для начала реализуйте функцию `get_book_data`, которая будет получать данные о книге с одной страницы (например, с [этой](http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html)). Соберите всю информацию, включая название, цену, рейтинг, количество в наличии, описание и дополнительные характеристики из таблицы Product Information. Результат достаточно вернуть в виде словаря.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8** — помимо качественно написанного кода важно также документировать функции по стандарту:\n",
    "* кратко описать, что она делает и для чего нужна;\n",
    "* какие входные аргументы принимает, какого они типа и что означают по смыслу;\n",
    "* аналогично описать возвращаемые значения.\n",
    "\n",
    "*P. S. Состав, количество аргументов функции и тип возвращаемого значения можете менять как вам удобно. То, что написано ниже в шаблоне — лишь пример.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "UfD2vAjHkEoS",
   "metadata": {
    "id": "UfD2vAjHkEoS"
   },
   "outputs": [],
   "source": [
    " def get_book_data(book_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Собирает данные о книге с заданной страницы.\n",
    "\n",
    "    Функция загружает HTML-контент страницы книги, извлекает информацию:\n",
    "    название, цену, рейтинг, количество в наличии, описание и таблицу характеристик.\n",
    "\n",
    "    Аргументы:\n",
    "        book_url (str): URL-адрес страницы книги для парсинга.\n",
    "\n",
    "    Возвращает:\n",
    "        dict: Словарь с ключами:\n",
    "            - 'title' (str): Название книги.\n",
    "            - 'price' (str): Цена книги (включая символ валюты).\n",
    "            - 'rating' (str): Рейтинг книги в текстовом формате (например, 'One').\n",
    "            - 'availability' (int): Количество доступных к покупке экземпляров.\n",
    "            - 'description' (str): Описание книги или пустая строка, если отсутствует.\n",
    "            - 'product_information' (dict): Дополнительные характеристики из таблицы.\n",
    "    \"\"\"\n",
    "\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    response = requests.get(book_url)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.find('h1').text\n",
    "\n",
    "    price = soup.find('p', class_='price_color').text\n",
    "\n",
    "    rating_tag = soup.find('p', class_='star-rating')\n",
    "    rating = rating_tag['class'][1] if rating_tag else None\n",
    "\n",
    "    availability_text = soup.find('p', class_='instock availability').text\n",
    "    availability = int(re.search(r'\\d+', availability_text).group())\n",
    "\n",
    "    description_tag = soup.find('div', id='product_description')\n",
    "    description = description_tag.find_next_sibling('p').text if description_tag else ''\n",
    "\n",
    "    product_table = soup.find('table', class_='table table-striped')\n",
    "    product_info = {}\n",
    "    for row in product_table.find_all('tr'):\n",
    "        header = row.find('th').text\n",
    "        value = row.find('td').text\n",
    "        product_info[header] = value\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'rating': rating,\n",
    "        'availability': availability,\n",
    "        'description': description,\n",
    "        'product_information': product_info\n",
    "    }\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "moRSO9Itp1LT",
   "metadata": {
    "id": "moRSO9Itp1LT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'A Light in the Attic',\n",
       " 'price': '£51.77',\n",
       " 'rating': 'Three',\n",
       " 'availability': 22,\n",
       " 'description': \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\",\n",
       " 'product_information': {'UPC': 'a897fe39b1053632',\n",
       "  'Product Type': 'Books',\n",
       "  'Price (excl. tax)': '£51.77',\n",
       "  'Price (incl. tax)': '£51.77',\n",
       "  'Tax': '£0.00',\n",
       "  'Availability': 'In stock (22 available)',\n",
       "  'Number of reviews': '0'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Используйте для самопроверки\n",
    "book_url = 'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "get_book_data(book_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u601Q4evosq6",
   "metadata": {
    "id": "u601Q4evosq6"
   },
   "source": [
    "## Задание 2. Сбор данных обо всех книгах (20 баллов)\n",
    "\n",
    "Создайте функцию `scrape_books`, которая будет проходиться по всем страницам из каталога (вида `http://books.toscrape.com/catalogue/page-{N}.html`) и осуществлять парсинг всех страниц в цикле, используя ранее написанную `get_book_data`.\n",
    "\n",
    "Добавьте аргумент-флаг, который будет отвечать за сохранение результата в файл: если он будет равен `True`, то информация сохранится в ту же папку в файл `books_data.txt`; иначе шаг сохранения будет пропущен.\n",
    "\n",
    "**Также не забывайте про соблюдение PEP-8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "kk78l6oDkdxl",
   "metadata": {
    "id": "kk78l6oDkdxl"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "\n",
    "def get_book_data(book_url: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Собирает данные о книге с заданной страницы.\n",
    "    \n",
    "    Аргументы:\n",
    "        book_url (str): URL-адрес страницы книги для парсинга.\n",
    "    \n",
    "    Возвращает:\n",
    "        Dict: Словарь с данными о книге.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(book_url)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        title = soup.find('h1').text\n",
    "\n",
    "        price = soup.find('p', class_='price_color').text\n",
    "\n",
    "        rating_tag = soup.find('p', class_='star-rating')\n",
    "        rating = rating_tag['class'][1] if rating_tag else None\n",
    "\n",
    "        availability_text = soup.find('p', class_='instock availability').text\n",
    "        availability_match = re.search(r'\\d+', availability_text)\n",
    "        availability = int(availability_match.group()) if availability_match else 0\n",
    "\n",
    "        description_tag = soup.find('div', id='product_description')\n",
    "        description = description_tag.find_next_sibling('p').text if description_tag else ''\n",
    "\n",
    "        product_table = soup.find('table', class_='table table-striped')\n",
    "        product_info = {}\n",
    "        if product_table:\n",
    "            for row in product_table.find_all('tr'):\n",
    "                header = row.find('th')\n",
    "                value = row.find('td')\n",
    "                if header and value:\n",
    "                    product_info[header.text] = value.text\n",
    "\n",
    "        return {\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'rating': rating,\n",
    "            'availability': availability,\n",
    "            'description': description,\n",
    "            'product_information': product_info,\n",
    "            'url': book_url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при парсинге книги {book_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_book_links_from_page(page_url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Извлекает все ссылки на книги со страницы каталога.\n",
    "    \n",
    "    Аргументы:\n",
    "        page_url (str): URL страницы каталога.\n",
    "    \n",
    "    Возвращает:\n",
    "        List[str]: Список URL отдельных книг.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        book_links = []\n",
    "        books = soup.find_all('article', class_='product_pod')\n",
    "        \n",
    "        for book in books:\n",
    "            link_tag = book.find('h3').find('a')\n",
    "            if link_tag and 'href' in link_tag.attrs:\n",
    "                # Преобразуем относительную ссылку в абсолютную\n",
    "                relative_link = link_tag['href']\n",
    "                if 'catalogue/' in relative_link:\n",
    "                    absolute_link = f\"https://books.toscrape.com/catalogue/{relative_link.split('catalogue/')[-1]}\"\n",
    "                else:\n",
    "                    absolute_link = f\"https://books.toscrape.com/catalogue/{relative_link}\"\n",
    "                book_links.append(absolute_link)\n",
    "        \n",
    "        return book_links\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении ссылок со страницы {page_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_books(is_save: bool = False, max_workers: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Собирает данные о всех книгах с сайта Books to Scrape.\n",
    "    \n",
    "    Функция проходит по всем страницам каталога, извлекает ссылки на книги\n",
    "    и использует многопоточность для параллельного сбора данных о книгах.\n",
    "    \n",
    "    Аргументы:\n",
    "        is_save (bool): Если True, сохраняет результаты в файл books_data.txt\n",
    "        max_workers (int): Количество потоков для параллельной обработки\n",
    "    \n",
    "    Возвращает:\n",
    "        List[Dict]: Список словарей с данными о книгах\n",
    "    \"\"\"\n",
    "    base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "    \n",
    "    all_book_links = []\n",
    "    page_num = 1\n",
    "    \n",
    "    print(\"Сбор ссылок на книги...\")\n",
    "    while True:\n",
    "        page_url = base_url.format(page_num)\n",
    "        try:\n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Достигнут конец каталога на странице {page_num - 1}\")\n",
    "                break\n",
    "                \n",
    "            book_links = get_book_links_from_page(page_url)\n",
    "            if not book_links:\n",
    "                print(f\"На странице {page_num} не найдено книг\")\n",
    "                break\n",
    "                \n",
    "            all_book_links.extend(book_links)\n",
    "            print(f\"Страница {page_num}: найдено {len(book_links)} книг\")\n",
    "            page_num += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при обработке страницы {page_num}: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Всего найдено ссылок на книги: {len(all_book_links)}\")\n",
    "    \n",
    "    # Парсим данные о книгах с использованием многопоточности\n",
    "    print(\"Начинаем парсинг книг...\")\n",
    "    books_data = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Запускаем парсинг для всех книг\n",
    "        future_to_url = {executor.submit(get_book_data, url): url for url in all_book_links}\n",
    "        \n",
    "        for i, future in enumerate(concurrent.futures.as_completed(future_to_url)):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                book_data = future.result()\n",
    "                if book_data:\n",
    "                    books_data.append(book_data)\n",
    "                \n",
    "                # Выводим прогресс\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"Обработано {i + 1}/{len(all_book_links)} книг\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке книги {url}: {e}\")\n",
    "    \n",
    "    if is_save:\n",
    "        try:\n",
    "            with open('books_data.txt', 'w', encoding='utf-8') as f:\n",
    "                for book in books_data:\n",
    "                    f.write(json.dumps(book, ensure_ascii=False, indent=2) + '\\n' + '-'*50 + '\\n')\n",
    "            print(f\"Данные сохранены в файл books_data.txt\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при сохранении в файл: {e}\")\n",
    "    \n",
    "    print(f\"Парсинг завершен. Собрано данных о {len(books_data)} книгах\")\n",
    "    return books_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c456141-8ea0-4bd4-ab11-5315f7f5a360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сбор ссылок на книги...\n",
      "Страница 1: найдено 20 книг\n",
      "Страница 2: найдено 20 книг\n",
      "Страница 3: найдено 20 книг\n",
      "Страница 4: найдено 20 книг\n",
      "Страница 5: найдено 20 книг\n",
      "Страница 6: найдено 20 книг\n",
      "Страница 7: найдено 20 книг\n",
      "Страница 8: найдено 20 книг\n",
      "Страница 9: найдено 20 книг\n",
      "Страница 10: найдено 20 книг\n",
      "Страница 11: найдено 20 книг\n",
      "Страница 12: найдено 20 книг\n",
      "Страница 13: найдено 20 книг\n",
      "Страница 14: найдено 20 книг\n",
      "Страница 15: найдено 20 книг\n",
      "Страница 16: найдено 20 книг\n",
      "Страница 17: найдено 20 книг\n",
      "Страница 18: найдено 20 книг\n",
      "Страница 19: найдено 20 книг\n",
      "Страница 20: найдено 20 книг\n",
      "Страница 21: найдено 20 книг\n",
      "Страница 22: найдено 20 книг\n",
      "Страница 23: найдено 20 книг\n",
      "Страница 24: найдено 20 книг\n",
      "Страница 25: найдено 20 книг\n",
      "Страница 26: найдено 20 книг\n",
      "Страница 27: найдено 20 книг\n",
      "Страница 28: найдено 20 книг\n",
      "Страница 29: найдено 20 книг\n",
      "Страница 30: найдено 20 книг\n",
      "Страница 31: найдено 20 книг\n",
      "Страница 32: найдено 20 книг\n",
      "Страница 33: найдено 20 книг\n",
      "Страница 34: найдено 20 книг\n",
      "Страница 35: найдено 20 книг\n",
      "Страница 36: найдено 20 книг\n",
      "Страница 37: найдено 20 книг\n",
      "Страница 38: найдено 20 книг\n",
      "Страница 39: найдено 20 книг\n",
      "Страница 40: найдено 20 книг\n",
      "Страница 41: найдено 20 книг\n",
      "Страница 42: найдено 20 книг\n",
      "Страница 43: найдено 20 книг\n",
      "Страница 44: найдено 20 книг\n",
      "Страница 45: найдено 20 книг\n",
      "Страница 46: найдено 20 книг\n",
      "Страница 47: найдено 20 книг\n",
      "Страница 48: найдено 20 книг\n",
      "Страница 49: найдено 20 книг\n",
      "Страница 50: найдено 20 книг\n",
      "Достигнут конец каталога на странице 50\n",
      "Всего найдено ссылок на книги: 1000\n",
      "Начинаем парсинг книг...\n",
      "Обработано 10/1000 книг\n",
      "Обработано 20/1000 книг\n",
      "Обработано 30/1000 книг\n",
      "Обработано 40/1000 книг\n",
      "Обработано 50/1000 книг\n",
      "Обработано 60/1000 книг\n",
      "Обработано 70/1000 книг\n",
      "Обработано 80/1000 книг\n",
      "Обработано 90/1000 книг\n",
      "Обработано 100/1000 книг\n",
      "Обработано 110/1000 книг\n",
      "Обработано 120/1000 книг\n",
      "Обработано 130/1000 книг\n",
      "Обработано 140/1000 книг\n",
      "Обработано 150/1000 книг\n",
      "Обработано 160/1000 книг\n",
      "Обработано 170/1000 книг\n",
      "Обработано 180/1000 книг\n",
      "Обработано 190/1000 книг\n",
      "Обработано 200/1000 книг\n",
      "Обработано 210/1000 книг\n",
      "Обработано 220/1000 книг\n",
      "Обработано 230/1000 книг\n",
      "Обработано 240/1000 книг\n",
      "Обработано 250/1000 книг\n",
      "Обработано 260/1000 книг\n",
      "Обработано 270/1000 книг\n",
      "Обработано 280/1000 книг\n",
      "Обработано 290/1000 книг\n",
      "Обработано 300/1000 книг\n",
      "Обработано 310/1000 книг\n",
      "Обработано 320/1000 книг\n",
      "Обработано 330/1000 книг\n",
      "Обработано 340/1000 книг\n",
      "Обработано 350/1000 книг\n",
      "Обработано 360/1000 книг\n",
      "Обработано 370/1000 книг\n",
      "Обработано 380/1000 книг\n",
      "Обработано 390/1000 книг\n",
      "Обработано 400/1000 книг\n",
      "Обработано 410/1000 книг\n",
      "Обработано 420/1000 книг\n",
      "Обработано 430/1000 книг\n",
      "Обработано 440/1000 книг\n",
      "Обработано 450/1000 книг\n",
      "Обработано 460/1000 книг\n",
      "Обработано 470/1000 книг\n",
      "Обработано 480/1000 книг\n",
      "Обработано 490/1000 книг\n",
      "Обработано 500/1000 книг\n",
      "Обработано 510/1000 книг\n",
      "Обработано 520/1000 книг\n",
      "Обработано 530/1000 книг\n",
      "Обработано 540/1000 книг\n",
      "Обработано 550/1000 книг\n",
      "Обработано 560/1000 книг\n",
      "Ошибка при парсинге книги https://books.toscrape.com/catalogue/greek-mythic-history_698/index.html: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: /catalogue/greek-mythic-history_698/index.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002096D5E4D10>, 'Connection to books.toscrape.com timed out. (connect timeout=None)'))\n",
      "Обработано 570/1000 книг\n",
      "Обработано 580/1000 книг\n",
      "Обработано 590/1000 книг\n",
      "Обработано 600/1000 книг\n",
      "Обработано 610/1000 книг\n",
      "Обработано 620/1000 книг\n",
      "Обработано 630/1000 книг\n",
      "Обработано 640/1000 книг\n",
      "Обработано 650/1000 книг\n",
      "Обработано 660/1000 книг\n",
      "Обработано 670/1000 книг\n",
      "Обработано 680/1000 книг\n",
      "Обработано 690/1000 книг\n",
      "Обработано 700/1000 книг\n",
      "Обработано 710/1000 книг\n",
      "Обработано 720/1000 книг\n",
      "Обработано 730/1000 книг\n",
      "Обработано 740/1000 книг\n",
      "Обработано 750/1000 книг\n",
      "Обработано 760/1000 книг\n",
      "Обработано 770/1000 книг\n",
      "Обработано 780/1000 книг\n",
      "Обработано 790/1000 книг\n",
      "Обработано 800/1000 книг\n",
      "Обработано 810/1000 книг\n",
      "Обработано 820/1000 книг\n",
      "Обработано 830/1000 книг\n",
      "Обработано 840/1000 книг\n",
      "Обработано 850/1000 книг\n",
      "Обработано 860/1000 книг\n",
      "Обработано 870/1000 книг\n",
      "Обработано 880/1000 книг\n",
      "Обработано 890/1000 книг\n",
      "Обработано 900/1000 книг\n",
      "Обработано 910/1000 книг\n",
      "Обработано 920/1000 книг\n",
      "Обработано 930/1000 книг\n",
      "Обработано 940/1000 книг\n",
      "Обработано 950/1000 книг\n",
      "Обработано 960/1000 книг\n",
      "Обработано 970/1000 книг\n",
      "Обработано 980/1000 книг\n",
      "Обработано 990/1000 книг\n",
      "Обработано 1000/1000 книг\n",
      "Данные сохранены в файл books_data.txt\n",
      "Парсинг завершен. Собрано данных о 999 книгах\n",
      "Тип результата: <class 'list'>, Количество книг: 999\n",
      "Время выполнения: 173.71 секунд\n",
      "\n",
      "Первые 3 книги:\n",
      "1. A Light in the Attic - £51.77\n",
      "2. Set Me Free - £17.46\n",
      "3. In Her Wake - £12.84\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "res = scrape_books(is_save=True, max_workers=30)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Тип результата: {type(res)}, Количество книг: {len(res)}\")\n",
    "print(f\"Время выполнения: {end_time - start_time:.2f} секунд\")\n",
    "if res:\n",
    "    print(\"\\nПервые 3 книги:\")\n",
    "    for i, book in enumerate(res[:3]):\n",
    "        print(f\"{i+1}. {book['title']} - {book['price']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5fd728nl8a8",
   "metadata": {
    "id": "z5fd728nl8a8"
   },
   "source": [
    "## Задание 3. Настройка регулярной выгрузки (10 баллов)\n",
    "\n",
    "Настройте автоматический запуск функции сбора данных каждый день в 19:00.\n",
    "Для автоматизации используйте библиотеку `schedule`. Функция должна запускаться в указанное время и сохранять обновленные данные в текстовый файл.\n",
    "\n",
    "\n",
    "\n",
    "Бесконечный цикл должен обеспечивать постоянное ожидание времени для запуска задачи и выполнять ее по расписанию. Однако чтобы не перегружать систему, стоит подумать о том, чтобы выполнять проверку нужного времени не постоянно, а раз в какой-то промежуток. В этом вам может помочь `time.sleep(...)`.\n",
    "\n",
    "Проверьте работоспособность кода локально на любом времени чч:мм.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b4b7085-e56d-479c-bb9a-3ac37c1ad44c",
   "metadata": {
    "id": "SajRRCj4n8BZ"
   },
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('books_scraper.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def scheduled_scraping():\n",
    "    \"\"\"\n",
    "    Функция для выполнения ежедневного сбора данных.\n",
    "    \"\"\"\n",
    "    logging.info(\"Запуск ежедневного сбора данных...\")\n",
    "    \n",
    "    try:\n",
    "        # Запускаем функцию скрапинга\n",
    "        books_data = scrape_books(is_save=False, max_workers=8)\n",
    "        \n",
    "        # Сохраняем данные с временной меткой\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f\"books_data_{timestamp}.txt\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Дата выгрузки: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Количество книг: {len(books_data)}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            \n",
    "            for book in books_data:\n",
    "                f.write(f\"Название: {book.get('title', 'N/A')}\\n\")\n",
    "                f.write(f\"Цена: {book.get('price', 'N/A')}\\n\")\n",
    "                f.write(f\"Рейтинг: {book.get('rating', 'N/A')}\\n\")\n",
    "                f.write(f\"В наличии: {book.get('availability', 0)} шт.\\n\")\n",
    "                f.write(f\"URL: {book.get('url', 'N/A')}\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        logging.info(f\"Данные успешно сохранены в файл: {filename}\")\n",
    "        logging.info(f\"Собрано {len(books_data)} книг\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ошибка при выполнении сбора данных: {e}\")\n",
    "\n",
    "def run_scheduler(schedule_time=\"19:00\"):\n",
    "    \"\"\"\n",
    "    Запускает планировщик задач для ежедневного выполнения в указанное время.\n",
    "    \n",
    "    Аргументы:\n",
    "        schedule_time (str): Время запуска в формате \"ЧЧ:ММ\" (по умолчанию \"19:00\")\n",
    "    \"\"\"\n",
    "    # Настраиваем расписание\n",
    "    schedule.every().day.at(schedule_time).do(scheduled_scraping)\n",
    "    \n",
    "    logging.info(f\"Планировщик запущен. Ежедневный сбор данных в {schedule_time}\")\n",
    "    print(f\"Планировщик запущен. Ежедневный сбор данных в {schedule_time}\")\n",
    "    print(\"Для остановки нажмите Ctrl+C\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Проверяем расписание каждые 60 секунд\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)\n",
    "        except KeyboardInterrupt:\n",
    "            logging.info(\"Планировщик остановлен пользователем\")\n",
    "            print(\"Планировщик остановлен\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка в планировщике: {e}\")\n",
    "            time.sleep(60)\n",
    "\n",
    "def start_daily_scraping(background=True, schedule_time=\"19:00\"):\n",
    "    \"\"\"\n",
    "    Запускает ежедневный сбор данных в указанное время.\n",
    "    \n",
    "    Аргументы:\n",
    "        background (bool): Если True, запускает в фоновом потоке\n",
    "        schedule_time (str): Время запуска в формате \"ЧЧ:ММ\" (по умолчанию \"19:00\")\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Ежедневный сбор данных Books to Scrape\")\n",
    "    print(f\"Время выполнения: {schedule_time}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if background:\n",
    "        # Запуск в отдельном потоке\n",
    "        scheduler_thread = threading.Thread(target=run_scheduler, args=(schedule_time,), daemon=True)\n",
    "        scheduler_thread.start()\n",
    "        logging.info(f\"Планировщик запущен в фоновом режиме. Время выполнения: {schedule_time}\")\n",
    "        return scheduler_thread\n",
    "    else:\n",
    "        # Запуск в основном потоке\n",
    "        run_scheduler(schedule_time)\n",
    "\n",
    "def start_with_time(hours, minutes):\n",
    "    \"\"\"\n",
    "    Запускает планировщик с указанным временем.\n",
    "    \n",
    "    Аргументы:\n",
    "        hours (int): Часы (0-23)\n",
    "        minutes (int): Минуты (0-59)\n",
    "    \"\"\"\n",
    "    # Форматируем время в нужный формат\n",
    "    schedule_time = f\"{hours:02d}:{minutes:02d}\"\n",
    "    \n",
    "    print(f\"Запуск планировщика с временем: {schedule_time}\")\n",
    "    start_daily_scraping(background=False, schedule_time=schedule_time)\n",
    "\n",
    "# Запуск планировщика с временем по умолчанию (19:00)\n",
    "def start_default():\n",
    "    \"\"\"\n",
    "    Запускает планировщик со временем по умолчанию (19:00).\n",
    "    \"\"\"\n",
    "    start_daily_scraping(background=False, schedule_time=\"19:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee419e3d-4348-43ef-8284-9b59ab7d7276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:04:57,266 - INFO - Планировщик запущен. Ежедневный сбор данных в 23:05\n",
      "2025-11-09 23:04:57,268 - INFO - Запуск ежедневного сбора данных...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск планировщика с временем: 23:05\n",
      "============================================================\n",
      "Ежедневный сбор данных Books to Scrape\n",
      "Время выполнения: 23:05\n",
      "============================================================\n",
      "Планировщик запущен. Ежедневный сбор данных в 23:05\n",
      "Для остановки нажмите Ctrl+C\n",
      "Сбор ссылок на книги...\n",
      "Страница 1: найдено 20 книг\n",
      "Страница 2: найдено 20 книг\n",
      "Страница 3: найдено 20 книг\n",
      "Страница 4: найдено 20 книг\n",
      "Страница 5: найдено 20 книг\n",
      "Страница 6: найдено 20 книг\n",
      "Страница 7: найдено 20 книг\n",
      "Страница 8: найдено 20 книг\n",
      "Страница 9: найдено 20 книг\n",
      "Страница 10: найдено 20 книг\n",
      "Страница 11: найдено 20 книг\n",
      "Страница 12: найдено 20 книг\n",
      "Страница 13: найдено 20 книг\n",
      "Страница 14: найдено 20 книг\n",
      "Страница 15: найдено 20 книг\n",
      "Страница 16: найдено 20 книг\n",
      "Страница 17: найдено 20 книг\n",
      "Страница 18: найдено 20 книг\n",
      "Страница 19: найдено 20 книг\n",
      "Страница 20: найдено 20 книг\n",
      "Страница 21: найдено 20 книг\n",
      "Страница 22: найдено 20 книг\n",
      "Страница 23: найдено 20 книг\n",
      "Страница 24: найдено 20 книг\n",
      "Страница 25: найдено 20 книг\n",
      "Страница 26: найдено 20 книг\n",
      "Страница 27: найдено 20 книг\n",
      "Страница 28: найдено 20 книг\n",
      "Страница 29: найдено 20 книг\n",
      "Страница 30: найдено 20 книг\n",
      "Страница 31: найдено 20 книг\n",
      "Страница 32: найдено 20 книг\n",
      "Страница 33: найдено 20 книг\n",
      "Страница 34: найдено 20 книг\n",
      "Страница 35: найдено 20 книг\n",
      "Страница 36: найдено 20 книг\n",
      "Страница 37: найдено 20 книг\n",
      "Страница 38: найдено 20 книг\n",
      "Страница 39: найдено 20 книг\n",
      "Страница 40: найдено 20 книг\n",
      "Страница 41: найдено 20 книг\n",
      "Страница 42: найдено 20 книг\n",
      "Страница 43: найдено 20 книг\n",
      "Страница 44: найдено 20 книг\n",
      "Страница 45: найдено 20 книг\n",
      "Страница 46: найдено 20 книг\n",
      "Страница 47: найдено 20 книг\n",
      "Страница 48: найдено 20 книг\n",
      "Страница 49: найдено 20 книг\n",
      "Страница 50: найдено 20 книг\n",
      "Достигнут конец каталога на странице 50\n",
      "Всего найдено ссылок на книги: 1000\n",
      "Начинаем парсинг книг...\n",
      "Обработано 10/1000 книг\n",
      "Обработано 20/1000 книг\n",
      "Обработано 30/1000 книг\n",
      "Обработано 40/1000 книг\n",
      "Обработано 50/1000 книг\n",
      "Обработано 60/1000 книг\n",
      "Обработано 70/1000 книг\n",
      "Обработано 80/1000 книг\n",
      "Обработано 90/1000 книг\n",
      "Обработано 100/1000 книг\n",
      "Обработано 110/1000 книг\n",
      "Обработано 120/1000 книг\n",
      "Обработано 130/1000 книг\n",
      "Обработано 140/1000 книг\n",
      "Обработано 150/1000 книг\n",
      "Обработано 160/1000 книг\n",
      "Обработано 170/1000 книг\n",
      "Обработано 180/1000 книг\n",
      "Обработано 190/1000 книг\n",
      "Обработано 200/1000 книг\n",
      "Обработано 210/1000 книг\n",
      "Обработано 220/1000 книг\n",
      "Обработано 230/1000 книг\n",
      "Обработано 240/1000 книг\n",
      "Обработано 250/1000 книг\n",
      "Обработано 260/1000 книг\n",
      "Обработано 270/1000 книг\n",
      "Обработано 280/1000 книг\n",
      "Обработано 290/1000 книг\n",
      "Обработано 300/1000 книг\n",
      "Обработано 310/1000 книг\n",
      "Обработано 320/1000 книг\n",
      "Обработано 330/1000 книг\n",
      "Обработано 340/1000 книг\n",
      "Обработано 350/1000 книг\n",
      "Обработано 360/1000 книг\n",
      "Обработано 370/1000 книг\n",
      "Обработано 380/1000 книг\n",
      "Обработано 390/1000 книг\n",
      "Обработано 400/1000 книг\n",
      "Обработано 410/1000 книг\n",
      "Обработано 420/1000 книг\n",
      "Обработано 430/1000 книг\n",
      "Обработано 440/1000 книг\n",
      "Обработано 450/1000 книг\n",
      "Обработано 460/1000 книг\n",
      "Обработано 470/1000 книг\n",
      "Обработано 480/1000 книг\n",
      "Обработано 490/1000 книг\n",
      "Обработано 500/1000 книг\n",
      "Обработано 510/1000 книг\n",
      "Обработано 520/1000 книг\n",
      "Обработано 530/1000 книг\n",
      "Обработано 540/1000 книг\n",
      "Обработано 550/1000 книг\n",
      "Обработано 560/1000 книг\n",
      "Обработано 570/1000 книг\n",
      "Обработано 580/1000 книг\n",
      "Обработано 590/1000 книг\n",
      "Обработано 600/1000 книг\n",
      "Обработано 610/1000 книг\n",
      "Обработано 620/1000 книг\n",
      "Обработано 630/1000 книг\n",
      "Обработано 640/1000 книг\n",
      "Обработано 650/1000 книг\n",
      "Обработано 660/1000 книг\n",
      "Обработано 670/1000 книг\n",
      "Обработано 680/1000 книг\n",
      "Обработано 690/1000 книг\n",
      "Обработано 700/1000 книг\n",
      "Обработано 710/1000 книг\n",
      "Обработано 720/1000 книг\n",
      "Обработано 730/1000 книг\n",
      "Обработано 740/1000 книг\n",
      "Обработано 750/1000 книг\n",
      "Обработано 760/1000 книг\n",
      "Обработано 770/1000 книг\n",
      "Обработано 780/1000 книг\n",
      "Обработано 790/1000 книг\n",
      "Обработано 800/1000 книг\n",
      "Обработано 810/1000 книг\n",
      "Обработано 820/1000 книг\n",
      "Обработано 830/1000 книг\n",
      "Обработано 840/1000 книг\n",
      "Обработано 850/1000 книг\n",
      "Обработано 860/1000 книг\n",
      "Обработано 870/1000 книг\n",
      "Обработано 880/1000 книг\n",
      "Обработано 890/1000 книг\n",
      "Обработано 900/1000 книг\n",
      "Обработано 910/1000 книг\n",
      "Обработано 920/1000 книг\n",
      "Обработано 930/1000 книг\n",
      "Обработано 940/1000 книг\n",
      "Обработано 950/1000 книг\n",
      "Обработано 960/1000 книг\n",
      "Обработано 970/1000 книг\n",
      "Обработано 980/1000 книг\n",
      "Обработано 990/1000 книг\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:09:08,539 - INFO - Данные успешно сохранены в файл: books_data_2025-11-09_23-09-08.txt\n",
      "2025-11-09 23:09:08,540 - INFO - Собрано 1000 книг\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 1000/1000 книг\n",
      "Парсинг завершен. Собрано данных о 1000 книгах\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:10:08,544 - INFO - Планировщик остановлен пользователем\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Планировщик остановлен\n"
     ]
    }
   ],
   "source": [
    "start_with_time(23,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9978610-c0cf-4826-a2b5-da2164a65967",
   "metadata": {
    "id": "XFiPtEyaoLxq"
   },
   "source": [
    "## Задание 4. Написание автотестов (15 баллов)\n",
    "\n",
    "Создайте минимум три автотеста для ключевых функций парсинга — например, `get_book_data` и `scrape_books`. Идеи проверок (можете использовать свои):\n",
    "\n",
    "* данные о книге возвращаются в виде словаря с нужными ключами;\n",
    "* список ссылок или количество собранных книг соответствует ожиданиям;\n",
    "* значения отдельных полей (например, `title`) корректны.\n",
    "\n",
    "Оформите тесты в отдельном скрипте `tests/test_scraper.py`, используйте библиотеку `pytest`. Убедитесь, что тесты проходят успешно при запуске из терминала командой `pytest`.\n",
    "\n",
    "Также выведите результат их выполнения в ячейке ниже.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lBFAw4b3z8QY",
   "metadata": {
    "id": "lBFAw4b3z8QY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.9, pytest-9.0.0, pluggy-1.6.0\n",
      "rootdir: C:\\Users\\Demen\\Desktop\\мфти\\дз\\hw3\\git\\books_scraper\\notebooks\n",
      "plugins: anyio-4.3.0, typeguard-4.2.1\n",
      "collected 0 items\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m ============================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: file or directory not found: test/test_scraper.py\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Ячейка для демонстрации работоспособности\n",
    "# Сам код напишите в отдельном скрипте\n",
    "!pytest tests/test_scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cRSQlHfRtOdN",
   "metadata": {
    "id": "cRSQlHfRtOdN"
   },
   "source": [
    "## Задание 5. Оформление проекта на GitHub и работа с Git (35 баллов)\n",
    "\n",
    "В этом задании нужно воспользоваться системой контроля версий Git и платформой GitHub для хранения и управления своим проектом. **Ссылку на свой репозиторий пришлите в форме для сдачи ответа.**\n",
    "\n",
    "### Пошаговая инструкция и задания\n",
    "\n",
    "**1. Установите Git на свой компьютер.**\n",
    "\n",
    "* Для Windows: [скачайте установщик](https://git-scm.com/downloads) и выполните установку.\n",
    "* Для macOS:\n",
    "\n",
    "  ```\n",
    "  brew install git\n",
    "  ```\n",
    "* Для Linux:\n",
    "\n",
    "  ```\n",
    "  sudo apt update\n",
    "  sudo apt install git\n",
    "  ```\n",
    "\n",
    "**2. Настройте имя пользователя и email.**\n",
    "\n",
    "Это нужно для подписи ваших коммитов, сделайте в терминале через `git config ...`.\n",
    "\n",
    "**3. Создайте аккаунт на GitHub**, если у вас его еще нет:\n",
    "[https://github.com](https://github.com)\n",
    "\n",
    "**4. Создайте новый репозиторий на GitHub:**\n",
    "\n",
    "* Найдите кнопку **New repository**.\n",
    "* Укажите название, краткое описание, выберите тип **Public** (чтобы мы могли проверить ДЗ).\n",
    "* Не ставьте галочку Initialize this repository with a README.\n",
    "\n",
    "**5. Создайте локальную папку с проектом.** Можно в терминале, можно через UI, это не имеет значения.\n",
    "\n",
    "**6. Инициализируйте Git в этой папке.** Здесь уже придется воспользоваться некоторой командой в терминале.\n",
    "\n",
    "**7. Привяжите локальный репозиторий к удаленному на GitHub.**\n",
    "\n",
    "**8. Создайте ветку разработки.** По умолчанию вы будете находиться в ветке `main`, создайте и переключитесь на ветку `hw-books-parser`.\n",
    "\n",
    "**9. Добавьте в проект следующие файлы и папки:**\n",
    "\n",
    "* `scraper.py` — ваш основной скрипт для сбора данных.\n",
    "* `README.md` — файл с кратким описанием проекта:\n",
    "\n",
    "  * цель;\n",
    "  * инструкции по запуску;\n",
    "  * список используемых библиотек.\n",
    "* `requirements.txt` — файл со списком зависимостей, необходимых для проекта (не присылайте все из глобального окружения, создайте изолированную виртуальную среду, добавьте в нее все нужное для проекта и получите список библиотек через `pip freeze`).\n",
    "* `artifacts/` — папка с результатами парсинга (`books_data.txt` — полностью или его часть, если весь не поместится на GitHub).\n",
    "* `notebooks/` — папка с заполненным ноутбуком `HW_03_python_ds_2025.ipynb` и запущенными ячейками с выводами на экран.\n",
    "* `tests/` — папка с тестами на `pytest`, оформите их в формате скрипта(-ов) с расширением `.py`.\n",
    "* `.gitignore` — стандартный файл, который позволит исключить временные файлы при добавлении в отслеживаемые (например, `__pycache__/`, `.DS_Store`, `*.pyc`, `venv/` и др.).\n",
    "\n",
    "\n",
    "**10. Сделайте коммит.**\n",
    "\n",
    "**11. Отправьте свою ветку на GitHub.**\n",
    "\n",
    "**12. Создайте Pull Request:**\n",
    "\n",
    "* Перейдите в репозиторий на GitHub.\n",
    "* Нажмите кнопку **Compare & pull request**.\n",
    "* Укажите, что было добавлено, и нажмите **Create pull request**.\n",
    "\n",
    "**13. Выполните слияние Pull Request:**\n",
    "\n",
    "* Убедитесь, что нет конфликтов.\n",
    "* Нажмите **Merge pull request**, затем **Confirm merge**.\n",
    "\n",
    "**14. Скачайте изменения из основной ветки локально.**\n",
    "\n",
    "\n",
    "\n",
    "### Требования к итоговому репозиторию\n",
    "\n",
    "* Файл `scraper.py` с рабочим кодом парсера.\n",
    "* `README.md` с описанием проекта и инструкцией по запуску.\n",
    "* Папка `artifacts/` с результатом сбора данных (`.txt` файл).\n",
    "* Папка `tests/` с тестами на `pytest`.\n",
    "* Папка `notebooks/` с заполненным ноутбуком `HW_03_python_ds_2025.ipynb`.\n",
    "* Pull Request с комментарием из ветки `hw-books-parser` в ветку `main`.\n",
    "* Примерная структура:\n",
    "\n",
    "  ```\n",
    "  books_scraper/\n",
    "  ├── artifacts/\n",
    "  │   └── books_data.txt\n",
    "  ├── notebooks/\n",
    "  │   └── HW_03_python_ds_2025.ipynb\n",
    "  ├── scraper.py\n",
    "  ├── README.md\n",
    "  ├── tests/\n",
    "  │   └── test_scraper.py\n",
    "  ├── .gitignore\n",
    "  └── requirements.txt\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
